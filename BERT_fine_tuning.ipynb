{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to **fine-tune BERT for a text classification task**.\n",
    "\n",
    "We'll be using the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) (SST-2) dataset of movie reviews and a smaller version of BERT - [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) - developed by HuggingFace.\n",
    "\n",
    "Our ultimate goal is to improve the accuracy of the model that we developed in the project [Text Classification with BERT](https://github.com/j-n-t/transformers/blob/master/text_classification_with_BERT.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading and checking the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the training dataset from our previous project, [Text Classification with BERT](https://github.com/j-n-t/transformers/blob/master/text_classification_with_BERT.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./.data/sst/tsv/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century 's...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of `` th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>singer\\/composer bryan adams contributes a sle...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yet the act is still charming here .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whether or not you 're enlightened by any of d...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  the rock is destined to be the 21st century 's...  positive\n",
       "1  the gorgeously elaborate continuation of `` th...  positive\n",
       "2  singer\\/composer bryan adams contributes a sle...  positive\n",
       "3               yet the act is still charming here .  positive\n",
       "4  whether or not you 're enlightened by any of d...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6920"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    3610\n",
       "negative    3310\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance reasons, we'll be using only **half of the training dataset** to fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.sample(frac=0.5, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e.t. works because its flabbergasting principa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from the dull , surreal ache of mortal awarene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a disoriented but occasionally disarming saga ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the only type of lives this glossy comedy-dram...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the affable maid in manhattan , jennifer lo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a must for fans of british cinema , if only be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>earnest and heartfelt but undernourished and p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>` anyone with a passion for cinema , and indee...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we never feel anything for these characters , ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>- style cross-country adventure ... it has spo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  e.t. works because its flabbergasting principa...  positive\n",
       "1  from the dull , surreal ache of mortal awarene...  positive\n",
       "2  a disoriented but occasionally disarming saga ...  positive\n",
       "3  the only type of lives this glossy comedy-dram...  negative\n",
       "4  in the affable maid in manhattan , jennifer lo...  positive\n",
       "5  a must for fans of british cinema , if only be...  positive\n",
       "6  earnest and heartfelt but undernourished and p...  positive\n",
       "7  ` anyone with a passion for cinema , and indee...  positive\n",
       "8  we never feel anything for these characters , ...  negative\n",
       "9  - style cross-country adventure ... it has spo...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1849\n",
       "negative    1611\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3460 reviews, from which 1849 are positive and 1611 are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use **DistilBERT to tokenize our reviews**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DistilBERT\n",
    "tokenizer_class, pretrained_weights = (DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# load pretrained tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenize reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by using the `encode` method to **determine the length of the longest review** after the special tokens have been added. These special tokens include the **\\[CLS\\]** token - id 101 - at the beginning of each review and the **\\[SEP\\]** token - id 102 - at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = df['review'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [101, 1041, 1012, 1056, 1012, 2573, 2138, 2049...\n",
       "1    [101, 2013, 1996, 10634, 1010, 16524, 12336, 1...\n",
       "2    [101, 1037, 4487, 21748, 25099, 2094, 2021, 56...\n",
       "3    [101, 1996, 2069, 2828, 1997, 3268, 2023, 1950...\n",
       "4    [101, 1999, 1996, 21358, 7011, 3468, 10850, 19...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(review) for review in input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rint(np.mean([len(review) for review in input_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest review has 80 tokens and our reviews have on average 25 tokens. We'll set **80 as the maximum length**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the `encode` method we can use the `encode_plus` method. This method will return a dictionary with the encoded sequence and some additional information.\n",
    "\n",
    "We set the **maximum length to 80, padd all the reviews and get an attention mask** for every review. We also set the `return_tensors` parameter to 'pt' to get PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = df['review'].apply((lambda x: tokenizer.encode_plus(x, add_special_tokens=True, \n",
    "                                                                   max_length=80, \n",
    "                                                                   pad_to_max_length=True, \n",
    "                                                                   return_attention_mask=True, \n",
    "                                                                   return_tensors='pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1041,  1012,  1056,  1012,  2573,  2138,  2049, 13109,  7875,\n",
       "           4059, 14083,  2075, 27928,  1010,  2403,  1011,  2095,  1011,  2214,\n",
       "           2728,  6097,  2532, 18533,  2239,  1010,  1020,  1011,  2095,  1011,\n",
       "           2214,  3881,  6287,  5974,  1998,  2184,  1011,  2095,  1011,  2214,\n",
       "           2888,  2726,  1010,  8054,  2149,  1997,  1996,  4598,  1997,  1996,\n",
       "           7968,  1010, 15536, 10431,  2098, 10367,  2013,  1037,  2521,  9497,\n",
       "           4774,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store our **input ids** and **attention masks** in two lists and convert them to the necessary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "\n",
    "for i, review in enumerate(encoded_dict):\n",
    "    \n",
    "    input_ids_list.append(encoded_dict[i]['input_ids'])\n",
    "    \n",
    "    attention_masks_list.append(encoded_dict[i]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,  1041,  1012,  1056,  1012,  2573,  2138,  2049, 13109,  7875,\n",
       "           4059, 14083,  2075, 27928,  1010,  2403,  1011,  2095,  1011,  2214,\n",
       "           2728,  6097,  2532, 18533,  2239,  1010,  1020,  1011,  2095,  1011,\n",
       "           2214,  3881,  6287,  5974,  1998,  2184,  1011,  2095,  1011,  2214,\n",
       "           2888,  2726,  1010,  8054,  2149,  1997,  1996,  4598,  1997,  1996,\n",
       "           7968,  1010, 15536, 10431,  2098, 10367,  2013,  1037,  2521,  9497,\n",
       "           4774,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[  101,  2013,  1996, 10634,  1010, 16524, 12336,  1997,  9801,  7073,\n",
       "          19391,  1037, 23751,  2839,  6533,  1012,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the list of tensors\n",
    "\n",
    "input_ids = torch.cat(input_ids_list, dim=0)\n",
    "attention_masks = torch.cat(attention_masks_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1041,  1012,  ...,     0,     0,     0],\n",
       "        [  101,  2013,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  1037,  4487,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2035,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  1037, 20161,  ...,     0,     0,     0],\n",
       "        [  101,  1996,  4164,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our 3460 reviews and corresponding attention masks in the desired format. Let's convert our labels as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Convert labels to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1849\n",
       "negative    1611\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_num = {'label': {'positive': 1, 'negative': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(label_to_num, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1849\n",
       "0    1611\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are now PyTorch tensors. Let's check one example of what we have done so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Check original review and corresponding tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review:  e.t. works because its flabbergasting principals , 14-year-old robert macnaughton , 6-year-old drew barrymore and 10-year-old henry thomas , convince us of the existence of the wise , wizened visitor from a faraway planet .\n",
      "\n",
      "Token IDs: tensor([  101,  1041,  1012,  1056,  1012,  2573,  2138,  2049, 13109,  7875,\n",
      "         4059, 14083,  2075, 27928,  1010,  2403,  1011,  2095,  1011,  2214,\n",
      "         2728,  6097,  2532, 18533,  2239,  1010,  1020,  1011,  2095,  1011,\n",
      "         2214,  3881,  6287,  5974,  1998,  2184,  1011,  2095,  1011,  2214,\n",
      "         2888,  2726,  1010,  8054,  2149,  1997,  1996,  4598,  1997,  1996,\n",
      "         7968,  1010, 15536, 10431,  2098, 10367,  2013,  1037,  2521,  9497,\n",
      "         4774,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "\n",
      "\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "Label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print('Original review: ', df['review'][0]+'\\n')\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('\\n')\n",
    "print('Attention Mask:', attention_masks[0])\n",
    "print('\\n')\n",
    "print('Label:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reviews are now tokenized and padded, and for each one of them we have the corresponding token ids, attention mask and label stored as PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to **split our dataset into a training set and a validation set**. This will be our step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create dataset made of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1041,  1012,  1056,  1012,  2573,  2138,  2049, 13109,  7875,\n",
       "          4059, 14083,  2075, 27928,  1010,  2403,  1011,  2095,  1011,  2214,\n",
       "          2728,  6097,  2532, 18533,  2239,  1010,  1020,  1011,  2095,  1011,\n",
       "          2214,  3881,  6287,  5974,  1998,  2184,  1011,  2095,  1011,  2214,\n",
       "          2888,  2726,  1010,  8054,  2149,  1997,  1996,  4598,  1997,  1996,\n",
       "          7968,  1010, 15536, 10431,  2098, 10367,  2013,  1037,  2521,  9497,\n",
       "          4774,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, our dataset stores a tuple of PyTorch tensors of the form (input_ids, attention_mask, label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2422 labeled reviews for training\n",
      "1038 labeled reviews for validation\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'{train_size} labeled reviews for training')\n",
    "print(f'{val_size} labeled reviews for validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a training dataset consisting of 70% of our data and a validation dataset with the other 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we should do is to **create a generator in order not to load the entire dataset into memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a training generator and a validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "# A.3 of https://arxiv.org/pdf/1810.04805.pdf\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              sampler = RandomSampler(train_dataset), # select batches randomly\n",
    "                              batch_size = batch_size)\n",
    "\n",
    "# for validation the order doesn't matter - we'll read them sequentially\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            sampler = SequentialSampler(val_dataset), # select batches sequentially\n",
    "                            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to start **training our model**. This will be our **step 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using [DistilBertForSequenceClassification](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification).\n",
    "\n",
    "Our model will be a **DistilBert Model transformer with a sequence classification head on top**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(pretrained_weights, \n",
    "                                                            num_labels = 2, # binary classification\n",
    "                                                            output_attentions = False, \n",
    "                                                            output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading our model, we need to **set our optimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Set optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW from the huggingface library\n",
    "# https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adamw-pytorch\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning BERT on a specific task, the authors recommend a learning rate (Adam) of 5e-5, 3e-5 or 2e-5, and to train the model for 2 to 4 epochs. You can check Appendix 3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) for more details.\n",
    "\n",
    "The [Adam paper](https://arxiv.org/pdf/1412.6980.pdf) suggests $10^{-8}$  as a good value for epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the total number of epochs and the total number of training steps, and create a [learning rate scheduler](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "\n",
    "# total number of training steps is [number of batches] x [number of epochs] \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen before, we have 2422 labeled reviews for training. Since we defined a batch size of 32, for each epoch we have (2422/32=75.7) 76 training steps.\n",
    "\n",
    "We can easily check this by getting the length of our `train_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are training our model for 4 epochs, we have a total number of training steps of 76 x 4 = 304."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the training of our model, we can also define **two helper functions** to **calculate the accuracy** of our model and to **format elapsed times**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the accuracy of our model\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    # extract prediction from logits\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    \n",
    "    # extract label from labels\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format elapsed times\n",
    "\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \n",
    "    # round to the nearest second\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally ready to **start training our model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++ Epoch 1 / 4 ++++++++++\n",
      "\n",
      "\n",
      "Training...\n",
      "\n",
      "  Batch 20  of  76  -->  Elapsed time: 0:02:49\n",
      "  Batch 40  of  76  -->  Elapsed time: 0:05:39\n",
      "  Batch 60  of  76  -->  Elapsed time: 0:08:32\n",
      "\n",
      "\n",
      "  Average training loss: 0.462\n",
      "  Elapsed time for training epoch: 0:10:45\n",
      "\n",
      "\n",
      "Validation...\n",
      "\n",
      "  Accuracy: 0.864\n",
      "  Average validation loss: 0.355\n",
      "  Elapsed time for validation: 0:01:27\n",
      "\n",
      "\n",
      "++++++++++ Epoch 2 / 4 ++++++++++\n",
      "\n",
      "\n",
      "Training...\n",
      "\n",
      "  Batch 20  of  76  -->  Elapsed time: 0:02:52\n",
      "  Batch 40  of  76  -->  Elapsed time: 0:05:43\n",
      "  Batch 60  of  76  -->  Elapsed time: 0:08:34\n",
      "\n",
      "\n",
      "  Average training loss: 0.222\n",
      "  Elapsed time for training epoch: 0:10:51\n",
      "\n",
      "\n",
      "Validation...\n",
      "\n",
      "  Accuracy: 0.863\n",
      "  Average validation loss: 0.350\n",
      "  Elapsed time for validation: 0:01:25\n",
      "\n",
      "\n",
      "++++++++++ Epoch 3 / 4 ++++++++++\n",
      "\n",
      "\n",
      "Training...\n",
      "\n",
      "  Batch 20  of  76  -->  Elapsed time: 0:02:51\n",
      "  Batch 40  of  76  -->  Elapsed time: 0:05:40\n",
      "  Batch 60  of  76  -->  Elapsed time: 0:08:31\n",
      "\n",
      "\n",
      "  Average training loss: 0.135\n",
      "  Elapsed time for training epoch: 0:10:47\n",
      "\n",
      "\n",
      "Validation...\n",
      "\n",
      "  Accuracy: 0.875\n",
      "  Average validation loss: 0.373\n",
      "  Elapsed time for validation: 0:01:24\n",
      "\n",
      "\n",
      "++++++++++ Epoch 4 / 4 ++++++++++\n",
      "\n",
      "\n",
      "Training...\n",
      "\n",
      "  Batch 20  of  76  -->  Elapsed time: 0:02:52\n",
      "  Batch 40  of  76  -->  Elapsed time: 0:05:43\n",
      "  Batch 60  of  76  -->  Elapsed time: 0:08:39\n",
      "\n",
      "\n",
      "  Average training loss: 0.094\n",
      "  Elapsed time for training epoch: 0:10:52\n",
      "\n",
      "\n",
      "Validation...\n",
      "\n",
      "  Accuracy: 0.881\n",
      "  Average validation loss: 0.387\n",
      "  Elapsed time for validation: 0:01:22\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Total training time 0:48:53 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# list to store training and validation loss, accuracy and elapsed times\n",
    "training_stats = []\n",
    "\n",
    "# time at the beginning of training to measure total training time\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    # TRAINING\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'++++++++++ Epoch {epoch+1} / {epochs} ++++++++++')\n",
    "    print('\\n')\n",
    "    print('Training...'+'\\n')\n",
    "\n",
    "    # time at the beginning of training epoch to measure this epoch training time\n",
    "    t0 = time.time()\n",
    "\n",
    "    # reset the total loss for this epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update every 20 batches\n",
    "        if step % 20 == 0 and not step == 0:\n",
    "            # calculate elapsed time\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # print progress\n",
    "            print(f'  Batch {step}  of  {len(train_dataloader)}  -->  Elapsed time: {elapsed}')\n",
    "\n",
    "        # unpack this training batch from our dataloader\n",
    "        b_input_ids = batch[0] # input ids\n",
    "        b_input_masks = batch[1] # attention masks\n",
    "        b_labels = batch[2] # labels\n",
    "\n",
    "        # clear any previously calculated gradients\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # perform a forward pass (evaluate the model on this training batch)\n",
    "        # https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification\n",
    "        # it returns the loss and the logits (classification scores before the activation function is applied)\n",
    "        loss, logits = model(input_ids = b_input_ids, \n",
    "                             attention_mask = b_input_masks, \n",
    "                             labels = b_labels)\n",
    "\n",
    "        # accumulate the training loss over all of the batches\n",
    "        # the .item() method just returns the value of the tensor as a standard Python number\n",
    "        # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the norm of the gradients to 1.0\n",
    "        # this is to help prevent the \"exploding gradients\" problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # average training loss over all of the batches\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # elapsed time for this epoch\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'  Average training loss: {avg_train_loss:.3f}')\n",
    "    print(f'  Elapsed time for training epoch: {training_time}')\n",
    "    \n",
    "    \n",
    "    # VALIDATION\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Validation...'+'\\n')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # create total_accuracy and total_val_loss variables\n",
    "    # and set them to zero\n",
    "    total_accuracy = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        \n",
    "        # unpack this training batch from our dataloader\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_masks = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        \n",
    "        # we don'need the gradients, so we don't build the computation graph\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # perform a forward pass\n",
    "            loss, logits = model(input_ids = b_input_ids, \n",
    "                                 attention_mask = b_input_masks, \n",
    "                                 labels = b_labels)\n",
    "            \n",
    "        # accumulate the validation loss over all of the batches\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        # convert logits and labels to numpy arrays\n",
    "        logits = logits.numpy()\n",
    "        labels = b_labels.numpy()\n",
    "\n",
    "        # calculate the accuracy for this batch of reviews\n",
    "        # and accumulate it over all batches\n",
    "        total_accuracy += flat_accuracy(logits, labels)\n",
    "        \n",
    "\n",
    "    # average validation accuracy after this epoch of training\n",
    "    avg_val_accuracy = total_accuracy / len(val_dataloader)\n",
    "\n",
    "    # average validation loss over all of the batches\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    \n",
    "    # elapsed time for the validation\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(f'  Accuracy: {avg_val_accuracy:.3f}')\n",
    "    print(f'  Average validation loss: {avg_val_loss:.3f}')\n",
    "    print(f'  Elapsed time for validation: {validation_time}')\n",
    "\n",
    "    # append statistics from this epoch to the training_stats list\n",
    "    training_stats.append({'epoch': epoch + 1, \n",
    "                           'Training Loss': avg_train_loss, \n",
    "                           'Validation Loss': avg_val_loss, \n",
    "                           'Accuracy': avg_val_accuracy, \n",
    "                           'Training Time': training_time, \n",
    "                           'Validation Time': validation_time})\n",
    "\n",
    "print('\\n')\n",
    "print('Training complete!')\n",
    "\n",
    "print(f'Total training time {format_time(time.time()-total_t0)} (h:mm:ss)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is trained and it is now time to evaluate it. This will be our final step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create dataframe with training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(data=training_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set epoch as index\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# display floats with 3 decimal places\n",
    "pd.set_option('precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.462</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0:10:45</td>\n",
       "      <td>0:01:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0:10:51</td>\n",
       "      <td>0:01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.135</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0:10:47</td>\n",
       "      <td>0:01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0:10:52</td>\n",
       "      <td>0:01:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Validation Loss  Accuracy Training Time Validation Time\n",
       "epoch                                                                        \n",
       "1              0.462            0.355     0.864       0:10:45         0:01:27\n",
       "2              0.222            0.350     0.863       0:10:51         0:01:25\n",
       "3              0.135            0.373     0.875       0:10:47         0:01:24\n",
       "4              0.094            0.387     0.881       0:10:52         0:01:22"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or model shows some signs of **overfitting** given that our **training loss keeps decreasing** but our **validation loss increases on epochs 3 and 4**.\n",
    "\n",
    "We get a **maximum accuracy value of 88.1%**, significantly higher than the value we've obtained in our previous project, [Text Classification with BERT](https://github.com/j-n-t/transformers/blob/master/text_classification_with_BERT.ipynb).\n",
    "\n",
    "Even if we consider our results for **epoch 2**, where the **validation loss is lower** overall, our accuracy value reveals that we've managed to successfully fine-tune our model for this classification task, even though our training set was relatively small.\n",
    "\n",
    "If you want to know more about BERT fine-tuning, please check the tutorial [BERT Fine-Tuning Tutorial with PyTorch](http://mccormickml.com/2019/07/22/BERT-fine-tuning/) by Chris McCormick and Nick Ryan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
